{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listes des imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Analyse  des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       img_path          label\n",
      "0  Advertisement/0000136188.jpg  Advertisement\n",
      "1  Advertisement/0000435350.jpg  Advertisement\n",
      "2  Advertisement/0000556056.jpg  Advertisement\n",
      "3  Advertisement/0030048095.jpg  Advertisement\n",
      "4  Advertisement/0030048989.jpg  Advertisement\n",
      "5  Advertisement/0030049569.jpg  Advertisement\n",
      "6    Advertisement/03496270.jpg  Advertisement\n",
      "7    Advertisement/03567810.jpg  Advertisement\n",
      "8    Advertisement/03722789.jpg  Advertisement\n",
      "9    Advertisement/04102204.jpg  Advertisement\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier text (affichage des 10 premier résultats)\n",
    "df=pd.read_csv(\"data/tobacco-lab_data_Tobacco3482.csv\") \n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier contient le pas vers l'image à associer dans la colonne *\"img_path\"* et sa classe dans la colonne *\"label\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data : 3482\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the statistics of each label\n",
    "sns.countplot(data=df,y='label')\n",
    "print('number of data :', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset contient 3482 images correspondant à des fichiers text collectés, séparés en 10 classe correspondantes respectivement à :\n",
    "\n",
    "- Advertissement\n",
    "- Email\n",
    "- Form\n",
    "- Letter\n",
    "- Memo\n",
    "- News\n",
    "- Note\n",
    "- Report\n",
    "- Resume\n",
    "- Scientic\n",
    "\n",
    "Ci-dessus l'on peut observer un graphique qui indique la proportion de chaque classe dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accéder au text\n",
    "path='data/Tobacco3482-OCR/'+str(df.img_path[0][:-3])+'txt'\n",
    "\n",
    "# Extraction du texte\n",
    "with open(path) as myfile:\n",
    "    content = myfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Mpertant as yar', 'sesiye teaetered cabiieess. Baely', '', 'doesn’) keow bea te', 'Bitton Aau-Fotne bl resin syste. Cant', '', 'viduiiliy crafted. Parenter', 'tiott, Most eapennese liste rn siichinng', '', 'Holimars. Costlicr of course', '', '      ', '  ', ' ', '', '“Has Oetenined -', '', 'Wainy: Thy', 'ie Hoel? h.', '', 'That Cia', '', 'Marg a Féme awe ii na eager ref Hizon a ol', '', ' ', '', ' ', '', 'Ra a', '', '6P9S70099']\n",
      "['Advertisement' 'Email' 'Form' 'Letter' 'Memo' 'News' 'Note' 'Report'\n",
      " 'Resume' 'Scientific']\n"
     ]
    }
   ],
   "source": [
    "# On extrait le texte des fichiers et l'on stocke chaque texte dans une liste text\n",
    "# On stocke les label de chaque text dans une liste label\n",
    "label=[]\n",
    "text=[]\n",
    "for i in range(df.shape[0]):\n",
    "    path=path='data/Tobacco3482-OCR/'+str(df.img_path[i][:-3])+'txt'\n",
    "    with open(path) as myfile:\n",
    "        content = myfile.readlines()\n",
    "        for j,e in enumerate(content):\n",
    "            #retrait des /n pour une meilleurs lisibilité et augmente les performances\n",
    "            content[j]=e.rstrip(\"\\n\")\n",
    "    text.append(content)\n",
    "    label.append(df.label[i])\n",
    "print(text[0])\n",
    "print(np.unique(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text          label\n",
      "0  [A Mpertant as yar, sesiye teaetered cabiieess...  Advertisement\n",
      "1  [TE che fitm, , m66400 7127, , KOOLS are the o...  Advertisement\n",
      "2  [so ARN Rr nr, , BWR Ga ||, , Vending Operator...  Advertisement\n",
      "3  [MARCH 24,19 VO — 3. Tersrearep, ,  , , ‘ yi i...  Advertisement\n",
      "4  [~, , Spend a milder moment qs, with Raleigh.,...  Advertisement\n",
      "5  [SR Onrel ules cee, Nee dss, , The one tales W...  Advertisement\n",
      "6  [&, BR. :, er non, , be 4, op Re eo eee ee eee...  Advertisement\n",
      "7  [{ae} BUIseaUT OY |, --uoanyf{sanosdun |., LIY...  Advertisement\n",
      "8  [. “So that’s he, any newspapel, “That's how,,...  Advertisement\n",
      "9  [BROWN & “WILLIAM ON:, , ;, mY To Man, , Marfa...  Advertisement\n",
      "(3482, 2)\n"
     ]
    }
   ],
   "source": [
    "# création d'un dataframe contenant chaque test et les labels associés\n",
    "cont=pd.DataFrame({'text':text, 'label':label})\n",
    "print(cont[:10])\n",
    "print(cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text          label\n",
      "0  A Mpertant as yar sesiye teaetered cabiieess. ...  Advertisement\n",
      "1  TE che fitm  m66400 7127  KOOLS are the only c...  Advertisement\n",
      "2  so ARN Rr nr  BWR Ga ||  Vending Operators    ...  Advertisement\n",
      "3  MARCH 24,19 VO — 3. Tersrearep     ‘ yi ills :...  Advertisement\n",
      "4  ~  Spend a milder moment qs with Raleigh.  = A...  Advertisement\n"
     ]
    }
   ],
   "source": [
    "# Texte ci-dessus stocker sous forme de séquences de mots\n",
    "# faire de text un text et non une sequence de mot\n",
    "cont['text']=[\" \".join(text) for text in cont['text'].values]\n",
    "print(cont[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Transformation en sac de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134          Form\n",
      "568          Email\n",
      "1706        Letter\n",
      "2619          News\n",
      "2098          Memo\n",
      "2268          Memo\n",
      "611          Email\n",
      "1533        Letter\n",
      "3195        Resume\n",
      "3293    Scientific\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Séparation des données en ensemble de test et d'apprentissage\n",
    "X_train,X_test, y_train,y_test = train_test_split(cont.text, cont.label, test_size=0.20, \n",
    "                                                random_state=42)\n",
    "\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne possède pas une enorme quantité de données (~3000) donc on choisit de prendre 20% des données pour former l'ensemble de test (voir cellule ci-dessus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb exemple apprentissage : (2785,)\n",
      "nb exemple de test: (697,)\n"
     ]
    }
   ],
   "source": [
    "print('nb exemple apprentissage :' ,X_train.shape)\n",
    "print('nb exemple de test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir séparé les différents ensemble on transforme nos texte en vecteur qui pourront être analyser/calculer par les modèles.\n",
    "\n",
    "Pour réaliser cela on utilise la méthode **_CountVectorizer_** de **sklearn**. Cette méthode effectue la convertion d'une collection de document texte en une matrice qui renvoie le nombre d'occurence de chaque symbole (du dico associé) pour chaque texte. On a fixe le nombre de paramètres des matrices à 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorisation\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vect = vectorizer.transform(X_train)\n",
    "X_test_vect= vectorizer.transform(X_test)\n",
    "\n",
    "print(X_test_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Représentation tf idf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise une transformation **_tf-idf_** sur les collection.\n",
    "\n",
    "La transformation **tf** correspond à calculer la fréquence de chaque mots (nombre d'occurence du mot/ sur le nombre total de mots) et on prend l'inverse, la **_tf-idf_** qui nous permet d'associer des poids qui permettrons de différencier les mot-clés des mots de liason ou pronom...etc (que l'on retouve avec de grande fréquences dans tous les documents car ils correspondent à la syntaxes de la langue mais n'apporte pas d'informations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation TF_IDF\n",
    "tf_transformer = TfidfTransformer().fit(X_train_vect)\n",
    "\n",
    "# transformation tf-idf des ensemble train, dev et test\n",
    "X_train_tf = tf_transformer.transform(X_train_vect)\n",
    "X_test_tf = tf_transformer.transform(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Classifieur naives bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les préscion obtenus sont : \n",
      "données test précision: 0.7302725968436155\n"
     ]
    }
   ],
   "source": [
    "# Entrainement avec un classifieur naif multinomial\n",
    "# train a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Definition du classifieur\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "# Entrainement du classifieur\n",
    "mnb.fit(X_train_vect, y_train)\n",
    "\n",
    "# Observasion de la précision\n",
    "acc_test=mnb.score(X_test_vect, y_test)\n",
    "y_pred=mnb.predict(X_test_vect)\n",
    "\n",
    "print('Les préscion obtenus sont : ')\n",
    "print('données test précision:',acc_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Précision de 73% avec le classifieur de naïves bayes et une représentation tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.73      0.67      0.70        57\n",
      "        Email       0.93      0.93      0.93       135\n",
      "         Form       0.81      0.82      0.81        88\n",
      "       Letter       0.75      0.72      0.74       122\n",
      "         Memo       0.60      0.73      0.66       109\n",
      "         News       0.69      0.74      0.71        34\n",
      "         Note       0.33      0.33      0.33        36\n",
      "       Report       0.59      0.56      0.57        48\n",
      "       Resume       1.00      1.00      1.00        15\n",
      "   Scientific       0.68      0.49      0.57        53\n",
      "\n",
      "    micro avg       0.73      0.73      0.73       697\n",
      "    macro avg       0.71      0.70      0.70       697\n",
      " weighted avg       0.73      0.73      0.73       697\n",
      "\n",
      "[[ 38   1   2   1   5   2   8   0   0   0]\n",
      " [  0 126   0   3   4   1   0   1   0   0]\n",
      " [  3   1  72   2   2   0   8   0   0   0]\n",
      " [  3   0   0  88  18   2   1   9   0   1]\n",
      " [  0   5   1  15  80   2   1   3   0   2]\n",
      " [  3   0   0   0   1  25   3   0   0   2]\n",
      " [  2   2   4   4  12   0  12   0   0   0]\n",
      " [  1   0   1   3   4   3   2  27   0   7]\n",
      " [  0   0   0   0   0   0   0   0  15   0]\n",
      " [  2   0   9   1   7   1   1   6   0  26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "label=np.unique(cont.label)\n",
    "target_names = [c for c in label ]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Classification with Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "#from nn_utils import TrainingHistory\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_FEATURES = 2000\n",
    "MAX_TEXT_LENGTH = 2000\n",
    "EMBED_SIZE  = 100\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_TEXT_LENGTH,EMBED_SIZE)(inp)\n",
    "    model = Dropout(0.3)(model)\n",
    "    model = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(10, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def model_2():\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_TEXT_LENGTH,EMBED_SIZE)(inp)\n",
    "    model = Dropout(0.3)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=4)(model)\n",
    "    \n",
    "    model = Dropout(0.25)(model)\n",
    "    model = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "     \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(10, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fit_predict(model, x_train, x_test, y):\n",
    "    \n",
    "    model.fit(x_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "2785 697\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 2000, 100)         200000    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2000, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 2000, 32)          16032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 500, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                160010    \n",
      "=================================================================\n",
      "Total params: 382,250\n",
      "Trainable params: 382,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "n_out = len(CLASSES_LIST)\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# Convert clas string to index\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "y_train = le.transform(y_train) \n",
    "y_test = le.transform(y_test) \n",
    "train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "x_vec_train, x_vec_test = get_train_test(X_train, X_test)\n",
    "print(len(x_vec_train), len(x_vec_test))\n",
    "\n",
    "# define the NN topology\n",
    "model = model_2()\n",
    "\n",
    "# Define training procedure\n",
    "#model.fit(x=x_vec_train, y=train_y_cat, batch_size=16, epochs=10, verbose=1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y_cat = np_utils.to_categorical(y_test, n_out)\n",
    "#score=model.evaluate(x_vec_test, test_y_cat)\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/20\n",
      "2506/2506 [==============================] - 29s 11ms/step - loss: 1.9102 - acc: 0.2977 - val_loss: 1.8088 - val_acc: 0.3369\n",
      "Epoch 2/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 1.4682 - acc: 0.4561 - val_loss: 1.2867 - val_acc: 0.5520\n",
      "Epoch 3/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.8755 - acc: 0.6748 - val_loss: 1.0771 - val_acc: 0.6201\n",
      "Epoch 4/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.5169 - acc: 0.8212 - val_loss: 1.1060 - val_acc: 0.6738\n",
      "Epoch 5/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.3403 - acc: 0.8755 - val_loss: 1.1876 - val_acc: 0.6738\n",
      "Epoch 6/20\n",
      "2506/2506 [==============================] - 29s 11ms/step - loss: 0.2179 - acc: 0.9290 - val_loss: 1.2167 - val_acc: 0.7133\n",
      "Epoch 7/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.1763 - acc: 0.9437 - val_loss: 1.3621 - val_acc: 0.6918\n",
      "Epoch 8/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.1489 - acc: 0.9541 - val_loss: 1.4408 - val_acc: 0.6810\n",
      "Epoch 9/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.1107 - acc: 0.9657 - val_loss: 1.5780 - val_acc: 0.6631\n",
      "Epoch 10/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.1014 - acc: 0.9677 - val_loss: 1.6175 - val_acc: 0.6774\n",
      "Epoch 11/20\n",
      "2506/2506 [==============================] - 29s 11ms/step - loss: 0.0783 - acc: 0.9792 - val_loss: 1.7845 - val_acc: 0.6738\n",
      "Epoch 12/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0933 - acc: 0.9685 - val_loss: 1.7212 - val_acc: 0.6918\n",
      "Epoch 13/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0958 - acc: 0.9665 - val_loss: 1.8937 - val_acc: 0.6559\n",
      "Epoch 14/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0867 - acc: 0.9721 - val_loss: 2.0388 - val_acc: 0.6738\n",
      "Epoch 15/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.1008 - acc: 0.9709 - val_loss: 2.0322 - val_acc: 0.6918\n",
      "Epoch 16/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0668 - acc: 0.9773 - val_loss: 1.9351 - val_acc: 0.6918\n",
      "Epoch 17/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0663 - acc: 0.9781 - val_loss: 1.9897 - val_acc: 0.7097\n",
      "Epoch 18/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0407 - acc: 0.9880 - val_loss: 2.0509 - val_acc: 0.7025\n",
      "Epoch 19/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0391 - acc: 0.9880 - val_loss: 1.9465 - val_acc: 0.7168\n",
      "Epoch 20/20\n",
      "2506/2506 [==============================] - 28s 11ms/step - loss: 0.0339 - acc: 0.9904 - val_loss: 2.0081 - val_acc: 0.6953\n"
     ]
    }
   ],
   "source": [
    "y_predicted=train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    y_pred.append(np.argmax(y_predicted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 9, 3, 4, 6, 1, 6, 4, 1, 2, 7, 3, 4, 0, 4, 3, 8, 4, 4, 4, 1,\n",
       "       6, 6, 6, 1, 3, 1, 2, 1, 4, 1, 4, 3, 6, 5, 4, 3, 0, 0, 7, 3, 2, 4,\n",
       "       4, 5, 4, 2, 0, 2, 4, 1, 1, 3, 3, 4, 7, 4, 3, 1, 4, 1, 2, 6, 2, 3,\n",
       "       5, 9, 1, 9, 2, 2, 2, 7, 9, 7, 1, 5, 3, 2, 1, 6, 4, 5, 4, 4, 1, 7,\n",
       "       3, 5, 9, 4, 7, 2, 3, 2, 6, 8, 6, 7, 2, 8, 4, 1, 8, 0, 7, 7, 1, 4,\n",
       "       7, 4, 0, 4, 2, 4, 2, 9, 6, 7, 1, 1, 4, 7, 6, 4, 0, 2, 6, 9, 3, 7,\n",
       "       2, 3, 0, 4, 9, 7, 4, 8, 4, 4, 3, 3, 9, 2, 2, 3, 4, 6, 2, 7, 5, 0,\n",
       "       4, 0, 2, 3, 8, 3, 7, 9, 9, 7, 9, 0, 2, 2, 1, 6, 9, 2, 2, 8, 2, 3,\n",
       "       0, 1, 5, 9, 4, 1, 4, 0, 1, 1, 0, 7, 1, 3, 1, 8, 2, 4, 7, 9, 4, 9,\n",
       "       4, 3, 2, 1, 2, 3, 2, 4, 1, 1, 1, 4, 4, 4, 0, 2, 9, 5, 9, 3, 3, 1,\n",
       "       9, 1, 9, 4, 8, 3, 3, 3, 2, 1, 7, 3, 3, 9, 2, 4, 7, 3, 4, 9, 6, 4,\n",
       "       4, 1, 0, 2, 0, 2, 9, 9, 1, 3, 1, 6, 4, 7, 1, 7, 6, 2, 3, 0, 9, 3,\n",
       "       3, 8, 1, 0, 4, 0, 1, 1, 4, 2, 1, 1, 0, 4, 6, 2, 3, 2, 4, 3, 2, 1,\n",
       "       1, 9, 0, 1, 9, 1, 9, 1, 2, 4, 2, 4, 4, 7, 3, 1, 1, 1, 5, 6, 7, 4,\n",
       "       2, 7, 1, 1, 5, 7, 7, 2, 9, 6, 3, 4, 4, 9, 4, 4, 3, 1, 1, 2, 6, 4,\n",
       "       3, 1, 7, 4, 2, 0, 3, 3, 1, 7, 4, 9, 5, 1, 1, 4, 7, 1, 6, 2, 4, 7,\n",
       "       7, 9, 2, 4, 5, 6, 2, 3, 7, 4, 4, 1, 4, 1, 5, 6, 1, 3, 1, 0, 8, 3,\n",
       "       2, 4, 1, 4, 7, 4, 4, 0, 4, 3, 1, 1, 3, 7, 1, 6, 4, 2, 0, 1, 0, 4,\n",
       "       2, 3, 4, 7, 7, 3, 4, 7, 8, 3, 7, 7, 5, 2, 6, 7, 3, 1, 2, 1, 4, 1,\n",
       "       1, 6, 0, 2, 6, 5, 9, 4, 2, 7, 2, 1, 3, 7, 2, 1, 5, 2, 3, 3, 4, 0,\n",
       "       0, 1, 3, 8, 1, 7, 0, 0, 3, 4, 4, 5, 4, 4, 1, 1, 2, 4, 3, 2, 5, 5,\n",
       "       0, 2, 1, 3, 4, 3, 9, 1, 4, 1, 9, 7, 3, 5, 3, 3, 9, 3, 0, 1, 7, 7,\n",
       "       3, 7, 7, 2, 1, 1, 7, 8, 4, 4, 1, 0, 3, 3, 1, 1, 4, 5, 5, 1, 2, 6,\n",
       "       8, 6, 1, 9, 4, 1, 2, 1, 4, 1, 3, 0, 7, 1, 4, 2, 3, 4, 2, 1, 2, 1,\n",
       "       6, 1, 2, 1, 7, 4, 2, 4, 6, 3, 5, 7, 7, 4, 1, 4, 6, 3, 3, 6, 7, 4,\n",
       "       7, 4, 6, 2, 7, 2, 4, 7, 1, 4, 2, 1, 3, 4, 4, 6, 8, 5, 0, 6, 6, 4,\n",
       "       4, 4, 7, 3, 1, 4, 4, 6, 6, 3, 4, 6, 4, 2, 4, 3, 2, 0, 5, 6, 4, 4,\n",
       "       0, 9, 1, 2, 0, 4, 3, 4, 3, 1, 7, 4, 7, 6, 4, 5, 1, 4, 4, 1, 3, 5,\n",
       "       7, 3, 4, 4, 1, 7, 3, 4, 4, 1, 5, 1, 9, 3, 1, 0, 3, 0, 2, 3, 2, 1,\n",
       "       1, 4, 1, 0, 4, 4, 4, 0, 1, 6, 3, 4, 1, 7, 1, 0, 7, 7, 9, 1, 7, 4,\n",
       "       9, 3, 3, 7, 1, 1, 2, 9, 2, 5, 4, 2, 0, 1, 9, 5, 9, 9, 3, 1, 3, 4,\n",
       "       4, 9, 7, 1, 4, 2, 0, 3, 0, 0, 4, 2, 1, 4, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.asarray(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7058823529411765\n",
      "p r f1:  70.6 70.59 70.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.53      0.57        57\n",
      "           1       0.96      0.87      0.91       135\n",
      "           2       0.83      0.80      0.81        88\n",
      "           3       0.80      0.61      0.69       122\n",
      "           4       0.68      0.87      0.77       109\n",
      "           5       0.69      0.65      0.67        34\n",
      "           6       0.44      0.56      0.49        36\n",
      "           7       0.39      0.58      0.47        48\n",
      "           8       0.94      1.00      0.97        15\n",
      "           9       0.46      0.40      0.42        53\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       697\n",
      "   macro avg       0.68      0.68      0.68       697\n",
      "weighted avg       0.73      0.71      0.71       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_pred, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1:  %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
